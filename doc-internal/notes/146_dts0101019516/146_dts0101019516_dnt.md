---

Copyright 2014 Mentor Graphics Corp.  All Rights Reserved.

---

# Verifier Performance Enhancement
### xtUML Project Design Note

1. Abstract
-----------
This note describes the investigation and selected solution to a reported
Verifier performance problem.

2. Document References
----------------------
[1] https://github.com/xtuml/internal/issues/146 - dts0101019516-Large number
    of functions make Verifier extremely slow

[2] CQ issue dts0101019516,  Large number of functions make Verifier extremely
    slow

[3] CQ issue dts0101035579,  Unexpected Breakpoint instances during Verifier
    startup.

[4] CQ issue dts0101035581,  Value instance population unnecessarily large
    during re-parsing.

[5] CQ issue dts0101035584,  Consider using hash set backed instance extents.

[6] CQ issue dts0100580449,  Search facility

3. Background
-------------

A customer has highlighted a case where significant execution speed degradation
is experienced. The issue headline is inaccurate, the problem occurs because
there are a large number of instances created by the model in question. This
causes performance to degrade significantly.

4. Requirements
---------------

Verifier performance should not degrade significantly as extra instances are
created at runtime.

5. Analysis
-----------

**5.1 Where clause investigation**

At first it was thought that where clauses being executed over very large
instance populations were responsible for the performance issue. To investigate
this, MC-Java was instrumented to add a call to size in every getMany* method
generated. The result was tested against a threshold which was temporarily
cached as a static constant in NonRootModelElement. This was done so that the
threshold might be easily changed. The instrumentation was tested by setting a
low threshold and lots of expected reports were obtained. Raising the threshold
above 500 elements resulted in only one report. This report occurred during
startup and could not be responsible for the performance problems. A separate
issue [3] is raised to investigate and resolve this detected anomaly.

**5.2 Memory leak**

Next a memory leak was considered. The BridgePoint instance population monitor
was used to look for spiraling numbers of model elements. No such rising
instance populations were observed. There were some interesting fluctuations in
some element populations which are discussed below[6.3]. However, the
investigation revealed no detectable memory leakage.

**5.3 Random program execution locus sampling**

This approach simply involves pushing the 'Suspend' debug button and noting
the stack top locus. If the stack top is repeatedly in the same area of the
code, we can hypothesize that program is spending a lot of time there. This
technique quickly showed the problem to be located in the element delete call
tree. More specifically, two calls were seen to be consuming very large amounts
of CPU resource; the InstanceList.contains(Object) and
InstanceList.remove(Object) methods. A quick web search confirmed that these
methods are indeed very inefficient in very large array based collections. The
problem is that for both methods, a linear search is performed from the start
of the array until the element instance to delete is found. To compound the
problem, identifying the correct instance involves calling
NonRootModelElement.equals(). This involves comparing the instances identifying
attributes. For the most part, these are UUIDs which would require the
comparison of four long values per attribute.

It was observed that the overhead described above was being seen in the code
for all meta-model elements. However, the largest instance populations were
those of Value and Runtime Value. These two element populations were around
40 thousand and 15 thousand instances respectively in the reproduction models
supplied by the customer. It turns out that these large instance populations
were causing problems at different stages of the Verifier session.

*5.3.1 Runtime Value element instances*

This large instance population is the one actually causing the reported
problems. The user model created around 500 object instances, each of which had
just under 30 attribute values. Each of these has a runtime value instance, so
the class instances alone accounted for over 13 thousand Runtime Value
instances. The remainder would be holding temporary variables on the stack.

*5.3.2 Value element instances*

This is an even larger element population. However, it does not affect Verifier
run time performance, because the problem is located in the delete call tree
and Value instances are static during the runtime session. Where Value
instances are deleted though, is during parsing, specifically during re-parsing.
It turns out this observation is significant and will be discussed further
below[6.3].

6. Design
---------

Two alternative solutions were considered:

**6.1 Switch InstanceList to use a HashSet**

By doing this, we remove the linear search and replace it with a hash based
lookup. This will be as efficient as the hash computation, the element is then
found by traversing (typically) a binary tree. This is much faster for most
cases than a linear search. Unfortunately, this approach must be discarded for
now due to some of the Undo/Redo and loading infrastructure being coupled to
the underlying Array implementation. An issue is raised to revisit this change
at a later date[5].

**6.2 Optimize the linear search**

The strategy here was inspired by the last observation in 5.3.1 above. In the
customer model, the 500 test instances were created at the beginning of the
run, so their Runtime Value instances will occur towards the beginning of the
instance extent array. The temporary Runtime Value instances by contrast will
all be created at higher index ranges in the array than these.

It is the temporary instances which typically get created and deleted much more
frequently than members of the instance populations, and it is these that take
the largest overhead during deletion, since the linear search must traverse to
the end of the array before finding the instance to remove.

Accordingly, we override the super.remove(Object) method to traverse backwards
through the instance extent array rather than forwards. This change has a
dramatic effect upon the performance; before the change, the user model takes
31 minutes and 30 seconds to perform the modeled tests. After the change, the
tests run in just 1 minute 21 seconds. Since most models will have this kind of
Runtime Value population it is safe to leave this optimization in place for all
customers.

We similarly override the InstanceList.contains() method to traverse the List
backwards when looking to see if the element is indeed contained within the
instance extent.

**6.3 Parse time**

The change proposed in the previous section is made to the common InstanceList
class. The change therefore affects the performance of all metamodel elements,
not just Verifier ones. If there are any cases where we have large element
populations and we are deleting elements at the **beginning** of the instance
population, then there will be an observed performance regression.

It was noted at 5.3.2 above that Value elements represent an even larger
element population than Runtime Value elements had. Value elements are deleted
during a re-parse and indeed, we observe a very significant performance
degradation when re-parsing a large model. In the case of the customer supplied
model, the time to restart the simulation (a delay attributable almost entirely
to re-parse time) goes from 1 minute 17 seconds to almost 12 minutes.

Given that re-starting is a less common use case than simulation itself, and
given that the regression is one third the magnitude of the gain this is an
acceptable price to pay. The ratios of gain to loss would likely be typical of
any user model. However, there are two additional enhancements that can reduce
the regression:

*6.3.1 Search backwards from the original creation index*

Because we only ever add new elements to the back of the instance list array,
we do not have to search linearly back from the very end of the array. Instead,
we can safely begin our search at the index that the instance was originally
created at since the instance must be at a lower index no matter what will have
happened to the instance population in the meantime. To do this, we cache the
index in NonRootModelElement during construction. This is done inside a
critical section, synchronized to the InstanceList extent cache. This is done
to guarantee that the index cached is always the one the instance was actually
cached at. At deletion time, the index is read using an accessor (the cached
value is private and cannot be modified from outside) and the instance search
commences from that value.

*6.3.2 Delete OAL instances in reverse order*

The code at the end of AllActivityModifier.initializeAllBodies() works through
the OAL instance population, removing stale Body instances (and everything
below them). This includes the very large Value instance population. By
working forwards through the list of Body instances, we guarantee that every
Value instance is at the very beginning of the instance array. This is the
worst case overhead for every Value instance. If we initialize the Bodies in
the reverse order that they were created, we avoid most of this traversal
overhead when deleting the stale Value instances.

It was noted that the population of Value instances doubles during a re-parse,
reaching a peak of 80 thousand in the case of the reproduction model. This
doubles the overhead and we should consider pre-deleting the stale body
instances, or at least do so after each action is successfully parsed. An issue
[4] is raised to track this enhancement.

The two enhancements proposed above together reduce the performance regression
from over 10 minutes to just over 4 minutes. Fixing [4] might well reduce this
regression to a level not noticeable to the user.

**6.4 Special case Runtime Value instances**

In this variation, two subtypes of InstanceList are created,
RuntimeInstanceList and StaticInstanceList. ModelRoot.getInstanceList() is
changed to check the type against a list of specified metamodel classes that
benefit from the reverse search. If the requested instance list is for a class
in this list, RuntimeInstanceList is returned. Otherwise, StaticInstanceList
is returned. The changes proposed above [6.2] are applied to the
RuntimeInstanceList class, while StaticInstanceList is written to search the
extent in normal forward direction. Because OAL subsystem instance extents are
not affected by this approach, the parse strategy and cached index changes are
not required.

The meta model classes specified for using a RuntimeInstanceList are all the
classes defined in the Runtime Value package in the OOA of OOA. That is,
Runtime Value, Structured Value, Simple Value, Array Value, Instance Reference
Value, Simple Core Value, Component Reference Value, Value In Structure, Value
In Array.

**6.5 Results**

| Alternatives:                  | Before |  6.2, 6.3  |   6.4   |
| ------------------------------ |:------:|:----------:|:-------:|
|Initial parse time              |   25s  |      21s   |    26s  |
|Run time                        |  1890s |      85s   |    99s  |
|Termination time                |   75s  |      57s   |    59s  |
|Restart time (re-parse overhead)|   77s  |     322s   |    82s  |
|Re-run time                     |  1850s |      81s   |    94s  |

The model used for comparison is the customers own model shared and referenced
from the issue [2]. The key results are the Run time and Re-run time rows in the
table above, a performance gain of almost 25x.

It is important to note that the changes proposed in 6.2 and 6.3 are
architectural. As such, they affect the performance of the tool as a whole, not
just Verifier. It is also important to note that the proposed changes tune the
architecture rather removing some performance block. There are therefore
performance losses as well as gains across the whole tool.  This is illustrated
in the table above by the Termination and Restart times. Termination happens to
gain from the tuning, but Restart takes a significant hit. In general however,
the gains outweigh the losses.

**6.6 Conclusion**

The solutions described in 6.2 and 6.3 provide the optimum runtime performance
improvement, there are undesirable Restart and Reparse overheads due to the need
to traverse the larger OAL instance populations described above in 6.3.2. The
special cased proposal in 6.4 is therefore preferred. An issue is raised [5] to
consider the more radical change proposed in 6.1.

7. Design Comments
------------------

There were unexpected failures in the RTO-Move test suite. This was traced to
the failure to remove deleted instances from the instance extent. The failure
was caused by violation of the assumptions made in 6.3.1. Recent changes to
support new search capabilities [6] removed code that restored element ordering
when a transaction was reverted. It is understood that the code was removed for
performance reasons. Instead, the reverted element is now placed at the end of
the instance extent, so we cannot now assume that the actual index will be less
than the original index.

The fix is to provide extra fall-back code in the InstanceList.remove() method
which traverses forward through the list from the original index if the element
was not found using the proposed backwards traversal policy. This path now
represents the worst case execution time (WCET) for element removal, but it is
felt that transaction reversion is sufficiently rare for the WCET case to be hit
only rarely.

8. Unit Test
------------

Because the proposed changes are architectural in nature, it is essential that
all unit tests be re-run for this issue.

Other than creating the very large instance populations which magnify this
issue, the user model contains no unique operating modes or configurations. We
already have tests which assure tool runtime performance, so no new tests are
required.

We do need to pay close attention to the performance results, however. A second
column shall be added to the unit test results template for this issue only.
This column shall show the 'before' run times for each of the JUnit tests. This
will make it easy to confirm that the performance deltas provide an overall
improvement in user experience.

End
---

